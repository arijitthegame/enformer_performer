{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arijitthegame/enformer_performer/blob/WIP/genformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrlMKyu_cE-4"
      },
      "source": [
        "#Just adding everything here\n",
        "# ==============================================================================\n",
        "\"\"\"Keras-based einsum layer.\n",
        "Copied from\n",
        "https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/dense_einsum.py.\n",
        "\"\"\"\n",
        "# pylint: disable=g-classes-have-attributes\n",
        "\n",
        "\n",
        "import inspect\n",
        "from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "_CHR_IDX = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\"]\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Text\")\n",
        "class DenseEinsum(tf.keras.layers.Layer):\n",
        "  \"\"\"A densely connected layer that uses tf.einsum as the backing computation.\n",
        "  This layer can perform einsum calculations of arbitrary dimensionality.\n",
        "  Arguments:\n",
        "    output_shape: Positive integer or tuple, dimensionality of the output space.\n",
        "    num_summed_dimensions: The number of dimensions to sum over. Standard 2D\n",
        "      matmul should use 1, 3D matmul should use 2, and so forth.\n",
        "    activation: Activation function to use. If you don't specify anything, no\n",
        "      activation is applied\n",
        "      (ie. \"linear\" activation: `a(x) = x`).\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
        "      matrix.\n",
        "    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "    activity_regularizer: Regularizer function applied to the output of the\n",
        "      layer (its \"activation\")..\n",
        "    kernel_constraint: Constraint function applied to the `kernel` weights\n",
        "      matrix.\n",
        "    bias_constraint: Constraint function applied to the bias vector.\n",
        "  Input shape:\n",
        "    N-D tensor with shape: `(batch_size, ..., input_dim)`. The most common\n",
        "      situation would be a 2D input with shape `(batch_size, input_dim)`.\n",
        "  Output shape:\n",
        "    N-D tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D\n",
        "      input with shape `(batch_size, input_dim)`, the output would have shape\n",
        "      `(batch_size, units)`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               output_shape,\n",
        "               num_summed_dimensions=1,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=\"glorot_uniform\",\n",
        "               bias_initializer=\"zeros\",\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "    super(DenseEinsum, self).__init__(**kwargs)\n",
        "    self._output_shape = output_shape if isinstance(\n",
        "        output_shape, (list, tuple)) else (output_shape,)\n",
        "    self._activation = tf.keras.activations.get(activation)\n",
        "    self._use_bias = use_bias\n",
        "    self._kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
        "    self._bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
        "    self._kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
        "    self._bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
        "    self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
        "    self._bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
        "    self._num_summed_dimensions = num_summed_dimensions\n",
        "    self._einsum_string = None\n",
        "\n",
        "  def _build_einsum_string(self, free_input_dims, bound_dims, output_dims):\n",
        "    input_str = \"\"\n",
        "    kernel_str = \"\"\n",
        "    output_str = \"\"\n",
        "    letter_offset = 0\n",
        "    for i in range(free_input_dims):\n",
        "      char = _CHR_IDX[i + letter_offset]\n",
        "      input_str += char\n",
        "      output_str += char\n",
        "\n",
        "    letter_offset += free_input_dims\n",
        "    for i in range(bound_dims):\n",
        "      char = _CHR_IDX[i + letter_offset]\n",
        "      input_str += char\n",
        "      kernel_str += char\n",
        "\n",
        "    letter_offset += bound_dims\n",
        "    for i in range(output_dims):\n",
        "      char = _CHR_IDX[i + letter_offset]\n",
        "      kernel_str += char\n",
        "      output_str += char\n",
        "\n",
        "    return input_str + \",\" + kernel_str + \"->\" + output_str\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    input_rank = input_shape.rank\n",
        "    free_input_dims = input_rank - self._num_summed_dimensions\n",
        "    output_dims = len(self._output_shape)\n",
        "\n",
        "    self._einsum_string = self._build_einsum_string(free_input_dims,\n",
        "                                                    self._num_summed_dimensions,\n",
        "                                                    output_dims)\n",
        "\n",
        "    # This is only saved for testing purposes.\n",
        "    self._kernel_shape = (\n",
        "        input_shape[free_input_dims:].concatenate(self._output_shape))\n",
        "\n",
        "    self._kernel = self.add_weight(\n",
        "        \"kernel\",\n",
        "        shape=self._kernel_shape,\n",
        "        initializer=self._kernel_initializer,\n",
        "        regularizer=self._kernel_regularizer,\n",
        "        constraint=self._kernel_constraint,\n",
        "        dtype=self.dtype,\n",
        "        trainable=True)\n",
        "    if self._use_bias:\n",
        "      self._bias = self.add_weight(\n",
        "          \"bias\",\n",
        "          shape=self._output_shape,\n",
        "          initializer=self._bias_initializer,\n",
        "          regularizer=self._bias_regularizer,\n",
        "          constraint=self._bias_constraint,\n",
        "          dtype=self.dtype,\n",
        "          trainable=True)\n",
        "    else:\n",
        "      self._bias = None\n",
        "    super(DenseEinsum, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        \"output_shape\":\n",
        "            self._output_shape,\n",
        "        \"num_summed_dimensions\":\n",
        "            self._num_summed_dimensions,\n",
        "        \"activation\":\n",
        "            tf.keras.activations.serialize(self._activation),\n",
        "        \"use_bias\":\n",
        "            self._use_bias,\n",
        "        \"kernel_initializer\":\n",
        "            tf.keras.initializers.serialize(self._kernel_initializer),\n",
        "        \"bias_initializer\":\n",
        "            tf.keras.initializers.serialize(self._bias_initializer),\n",
        "        \"kernel_regularizer\":\n",
        "            tf.keras.regularizers.serialize(self._kernel_regularizer),\n",
        "        \"bias_regularizer\":\n",
        "            tf.keras.regularizers.serialize(self._bias_regularizer),\n",
        "        \"activity_regularizer\":\n",
        "            tf.keras.regularizers.serialize(self._activity_regularizer),\n",
        "        \"kernel_constraint\":\n",
        "            tf.keras.constraints.serialize(self._kernel_constraint),\n",
        "        \"bias_constraint\":\n",
        "            tf.keras.constraints.serialize(self._bias_constraint)\n",
        "    }\n",
        "    base_config = super(DenseEinsum, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    ret = tf.einsum(self._einsum_string, inputs, self._kernel)\n",
        "    if self._use_bias:\n",
        "      ret += self._bias\n",
        "    if self._activation is not None:\n",
        "      ret = self._activation(ret)\n",
        "    return ret"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt6_w5nFfiyC"
      },
      "source": [
        "BIG_CONSTANT = 1e8\n",
        "\n",
        "\n",
        "def create_projection_matrix(m, d, seed=0, scaling=0, struct_mode=False):\n",
        "  r\"\"\"Constructs the matrix of random projections.\n",
        "  Constructs a matrix of random orthogonal projections. Each projection vector\n",
        "  has direction chosen uniformly at random and either deterministic length\n",
        "  \\sqrt{d} or length taken from the \\chi(d) distribution (in the latter case\n",
        "  marginal distributions of the projections are d-dimensional Gaussian vectors\n",
        "  with associated identity covariance matrix).\n",
        "  Args:\n",
        "    m: number of random projections.\n",
        "    d: dimensionality of each random projection.\n",
        "    seed: random seed used to construct projections.\n",
        "    scaling: 1 if all the random projections need to be renormalized to have\n",
        "      length \\sqrt{d}, 0 if the lengths of random projections should follow\n",
        "      \\chi(d) distribution.\n",
        "    struct_mode: if True then products of Givens rotations will be used to\n",
        "      construct random orthogonal matrix. This bypasses Gram-Schmidt\n",
        "      orthogonalization.\n",
        "  Returns:\n",
        "    The matrix of random projections of the shape [m, d].\n",
        "  \"\"\"\n",
        "  nb_full_blocks = int(m / d)\n",
        "  block_list = []\n",
        "  current_seed = seed\n",
        "  for _ in range(nb_full_blocks):\n",
        "    if struct_mode:\n",
        "      q = create_products_of_givens_rotations(d, seed)\n",
        "    else:\n",
        "      unstructured_block = tf.random.normal((d, d), seed=current_seed)\n",
        "      q, _ = tf.linalg.qr(unstructured_block)\n",
        "      q = tf.transpose(q)\n",
        "    block_list.append(q)\n",
        "    current_seed += 1\n",
        "  remaining_rows = m - nb_full_blocks * d\n",
        "  if remaining_rows > 0:\n",
        "    if struct_mode:\n",
        "      q = create_products_of_givens_rotations(d, seed)\n",
        "    else:\n",
        "      unstructured_block = tf.random.normal((d, d), seed=current_seed)\n",
        "      q, _ = tf.linalg.qr(unstructured_block)\n",
        "      q = tf.transpose(q)\n",
        "    block_list.append(q[0:remaining_rows])\n",
        "  final_matrix = tf.experimental.numpy.vstack(block_list)\n",
        "  current_seed += 1\n",
        "\n",
        "  if scaling == 0:\n",
        "    multiplier = tf.norm(tf.random.normal((m, d), seed=current_seed), axis=1)\n",
        "  elif scaling == 1:\n",
        "    multiplier = tf.math.sqrt(float(d)) * tf.ones((m))\n",
        "  else:\n",
        "    raise ValueError(\"Scaling must be one of {0, 1}. Was %s\" % scaling)\n",
        "\n",
        "  return tf.linalg.matmul(tf.linalg.diag(multiplier), final_matrix)\n",
        "\n",
        "\n",
        "def create_products_of_givens_rotations(dim, seed):\n",
        "  r\"\"\"Constructs a 2D-tensor which is a product of Givens random rotations.\n",
        "  Constructs a 2D-tensor of the form G_1 * ... * G_k, where G_i is a Givens\n",
        "  random rotation. The resulting tensor mimics a matrix taken uniformly at\n",
        "  random form the orthogonal group.\n",
        "  Args:\n",
        "    dim: number of rows/columns of the resulting 2D-tensor.\n",
        "    seed: random seed.\n",
        "  Returns:\n",
        "    The product of Givens random rotations.\n",
        "  \"\"\"\n",
        "  nb_givens_rotations = dim * int(math.ceil(math.log(float(dim))))\n",
        "  q = np.eye(dim, dim)\n",
        "  np.random.seed(seed)\n",
        "  for _ in range(nb_givens_rotations):\n",
        "    random_angle = math.pi * np.random.uniform()\n",
        "    random_indices = np.random.choice(dim, 2)\n",
        "    index_i = min(random_indices[0], random_indices[1])\n",
        "    index_j = max(random_indices[0], random_indices[1])\n",
        "    slice_i = q[index_i]\n",
        "    slice_j = q[index_j]\n",
        "    new_slice_i = math.cos(random_angle) * slice_i + math.sin(\n",
        "        random_angle) * slice_j\n",
        "    new_slice_j = -math.sin(random_angle) * slice_i + math.cos(\n",
        "        random_angle) * slice_j\n",
        "    q[index_i] = new_slice_i\n",
        "    q[index_j] = new_slice_j\n",
        "  return tf.cast(tf.constant(q), dtype=tf.float32)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6PrWBh2f3v3"
      },
      "source": [
        "def softmax_kernel_transformation(data,\n",
        "                                  is_query,\n",
        "                                  projection_matrix=None,\n",
        "                                  numerical_stabilizer=0.000001):\n",
        "  \"\"\"Computes random features for the softmax kernel using FAVOR+ mechanism.\n",
        "  Computes random features for the softmax kernel using FAVOR+ mechanism from\n",
        "  https://arxiv.org/pdf/2009.14794.pdf.\n",
        "  Args:\n",
        "    data: input data tensor of the shape [B, L, H, D], where: B - batch\n",
        "      dimension, L - attention dimensions, H - heads, D - features.\n",
        "    is_query: indicates whether input data is a query oor key tensor.\n",
        "    projection_matrix: random Gaussian matrix of shape [M, D], where M stands\n",
        "      for the number of random features and each D x D sub-block has pairwise\n",
        "      orthogonal rows.\n",
        "    numerical_stabilizer: small positive constant for numerical stability.\n",
        "  Returns:\n",
        "    Corresponding kernel feature map.\n",
        "  \"\"\"\n",
        "  data_normalizer = 1.0 / (\n",
        "      tf.math.sqrt(tf.math.sqrt(tf.dtypes.cast(data.shape[-1], tf.float32))))\n",
        "  data = data_normalizer * data\n",
        "  ratio = 1.0 / tf.math.sqrt(\n",
        "      tf.dtypes.cast(projection_matrix.shape[0], tf.float32))\n",
        "  data_dash = tf.einsum(\"blhd,md->blhm\", data, projection_matrix)\n",
        "  diag_data = tf.math.square(data)\n",
        "  diag_data = tf.math.reduce_sum(\n",
        "      diag_data, axis=tf.keras.backend.ndim(data) - 1)\n",
        "  diag_data = diag_data / 2.0\n",
        "  diag_data = tf.expand_dims(diag_data, axis=tf.keras.backend.ndim(data) - 1)\n",
        "  last_dims_t = (len(data_dash.shape) - 1,)\n",
        "  attention_dims_t = (len(data_dash.shape) - 3,)\n",
        "  if is_query:\n",
        "    data_dash = ratio * (\n",
        "        tf.math.exp(data_dash - diag_data - tf.math.reduce_max(\n",
        "            data_dash, axis=last_dims_t, keepdims=True)) + numerical_stabilizer)\n",
        "  else:\n",
        "    data_dash = ratio * (\n",
        "        tf.math.exp(data_dash - diag_data - tf.math.reduce_max(\n",
        "            data_dash, axis=last_dims_t + attention_dims_t, keepdims=True)) +\n",
        "        numerical_stabilizer)\n",
        "\n",
        "  return data_dash\n",
        "\n",
        "\n",
        "def noncausal_numerator(qs, ks, vs):\n",
        "  \"\"\"Computes not-normalized FAVOR noncausal attention AV.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "    vs: value tensor of the shape [L,B,H,D].\n",
        "  Returns:\n",
        "    Not-normalized FAVOR noncausal attention AV.\n",
        "  \"\"\"\n",
        "  kvs = tf.einsum(\"lbhm,lbhd->bhmd\", ks, vs)\n",
        "  return tf.einsum(\"lbhm,bhmd->lbhd\", qs, kvs)\n",
        "\n",
        "\n",
        "def noncausal_denominator(qs, ks):\n",
        "  \"\"\"Computes FAVOR normalizer in noncausal attention.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "  Returns:\n",
        "    FAVOR normalizer in noncausal attention.\n",
        "  \"\"\"\n",
        "  all_ones = tf.ones([ks.shape[0]])\n",
        "  ks_sum = tf.einsum(\"lbhm,l->bhm\", ks, all_ones)\n",
        "  return tf.einsum(\"lbhm,bhm->lbh\", qs, ks_sum)\n",
        "\n",
        "\n",
        "@tf.custom_gradient\n",
        "def causal_numerator(qs, ks, vs):\n",
        "  \"\"\"Computes not-normalized FAVOR causal attention A_{masked}V.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "    vs: value tensor of the shape [L,B,H,D].\n",
        "  Returns:\n",
        "    Not-normalized FAVOR causal attention A_{masked}V.\n",
        "  \"\"\"\n",
        "\n",
        "  result = []\n",
        "  sums = tf.zeros_like(tf.einsum(\"ijk,ijl->ijkl\", ks[0], vs[0]))\n",
        "\n",
        "  for index in range(qs.shape[0]):\n",
        "    sums = sums + tf.einsum(\"ijk,ijl->ijkl\", ks[index], vs[index])\n",
        "    result.append(tf.einsum(\"ijkl,ijk->ijl\", sums, qs[index])[None, Ellipsis])\n",
        "\n",
        "  result = tf.concat(result, axis=0)\n",
        "\n",
        "  def grad(res_grad):\n",
        "\n",
        "    grads = tf.zeros_like(tf.einsum(\"ijk,ijl->ijkl\", ks[0], vs[0]))\n",
        "\n",
        "    gr_sums = sums\n",
        "\n",
        "    q_grads = []\n",
        "    k_grads = []\n",
        "    v_grads = []\n",
        "\n",
        "    for index in range(qs.shape[0] - 1, -1, -1):\n",
        "\n",
        "      q_grads.append(\n",
        "          tf.einsum(\"ijkl,ijl->ijk\", gr_sums, res_grad[index])[None, Ellipsis])\n",
        "      grads = grads + tf.einsum(\"ijk,ijl->ijkl\", qs[index], res_grad[index])\n",
        "      k_grads.append(tf.einsum(\"ijkl,ijl->ijk\", grads, vs[index])[None, Ellipsis])\n",
        "      v_grads.append(tf.einsum(\"ijkl,ijk->ijl\", grads, ks[index])[None, Ellipsis])\n",
        "      gr_sums = gr_sums - tf.einsum(\"ijk,ijl->ijkl\", ks[index], vs[index])\n",
        "\n",
        "    q_grads = tf.concat(q_grads[::-1], axis=0)\n",
        "    k_grads = tf.concat(k_grads[::-1], axis=0)\n",
        "    v_grads = tf.concat(v_grads[::-1], axis=0)\n",
        "\n",
        "    return q_grads, k_grads, v_grads\n",
        "\n",
        "  return result, grad\n",
        "\n",
        "\n",
        "@tf.custom_gradient\n",
        "def causal_denominator(qs, ks):\n",
        "  \"\"\"Computes FAVOR normalizer in causal attention.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "  Returns:\n",
        "    FAVOR normalizer in causal attention.\n",
        "  \"\"\"\n",
        "\n",
        "  result = []\n",
        "  sums = tf.zeros_like(ks[0])\n",
        "\n",
        "  for index in range(qs.shape[0]):\n",
        "    sums = sums + ks[index]\n",
        "    result.append(tf.reduce_sum(qs[index] * sums, axis=2)[None, Ellipsis])\n",
        "\n",
        "  result = tf.concat(result, axis=0)\n",
        "\n",
        "  def grad(res_grad):\n",
        "\n",
        "    k_grad = tf.zeros_like(ks[0])\n",
        "\n",
        "    gr_sums = sums\n",
        "\n",
        "    q_grads = []\n",
        "    k_grads = []\n",
        "\n",
        "    for index in range(qs.shape[0] - 1, -1, -1):\n",
        "\n",
        "      q_grads.append(\n",
        "          tf.einsum(\"ijk,ij->ijk\", gr_sums, res_grad[index])[None, Ellipsis])\n",
        "      k_grad = k_grad + tf.einsum(\"ijk,ij->ijk\", qs[index], res_grad[index])\n",
        "      k_grads.append(k_grad[None, Ellipsis])\n",
        "      gr_sums = gr_sums - ks[index]\n",
        "\n",
        "    q_grads = tf.concat(q_grads[::-1], axis=0)\n",
        "    k_grads = tf.concat(k_grads[::-1], axis=0)\n",
        "\n",
        "    return q_grads, k_grads\n",
        "\n",
        "  return result, grad\n",
        "\n",
        "\n",
        "def favor_attention(query,\n",
        "                    key,\n",
        "                    value,\n",
        "                    kernel_transformation,\n",
        "                    causal,\n",
        "                    projection_matrix=None):\n",
        "  \"\"\"Computes FAVOR normalized attention.\n",
        "  Args:\n",
        "    query: query tensor.\n",
        "    key: key tensor.\n",
        "    value: value tensor.\n",
        "    kernel_transformation: transformation used to get finite kernel features.\n",
        "    causal: whether attention is causal or not.\n",
        "    projection_matrix: projection matrix to be used.\n",
        "  Returns:\n",
        "    FAVOR normalized attention.\n",
        "  \"\"\"\n",
        "  query_prime = kernel_transformation(query, True,\n",
        "                                      projection_matrix)  # [B,L,H,M]\n",
        "  key_prime = kernel_transformation(key, False, projection_matrix)  # [B,L,H,M]\n",
        "  query_prime = tf.transpose(query_prime, [1, 0, 2, 3])  # [L,B,H,M]\n",
        "  key_prime = tf.transpose(key_prime, [1, 0, 2, 3])  # [L,B,H,M]\n",
        "  value = tf.transpose(value, [1, 0, 2, 3])  # [L,B,H,D]\n",
        "\n",
        "  if causal:\n",
        "    av_attention = causal_numerator(query_prime, key_prime, value)\n",
        "    attention_normalizer = causal_denominator(query_prime, key_prime)\n",
        "  else:\n",
        "    av_attention = noncausal_numerator(query_prime, key_prime, value)\n",
        "    attention_normalizer = noncausal_denominator(query_prime, key_prime)\n",
        "  # TODO(kchoro): Add more comments.\n",
        "  av_attention = tf.transpose(av_attention, [1, 0, 2, 3])\n",
        "  attention_normalizer = tf.transpose(attention_normalizer, [1, 0, 2])\n",
        "  attention_normalizer = tf.expand_dims(attention_normalizer,\n",
        "                                        len(attention_normalizer.shape))\n",
        "  return av_attention / attention_normalizer\n",
        "\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  \"\"\"Multi-headed attention layer.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               hidden_size,\n",
        "               num_heads,\n",
        "               attention_dropout,\n",
        "               kernel_transformation=softmax_kernel_transformation,\n",
        "               numerical_stabilizer=0.001,\n",
        "               causal=False,\n",
        "               projection_matrix_type=None,\n",
        "               nb_random_features=0):\n",
        "    \"\"\"Initialize Attention.\n",
        "    Args:\n",
        "      hidden_size: int, output dim of hidden layer.\n",
        "      num_heads: int, number of heads to repeat the same attention structure.\n",
        "      attention_dropout: float, dropout rate inside attention for training.\n",
        "      kernel_transformation: transformation used to produce kernel features for\n",
        "        attention.\n",
        "      numerical_stabilizer: used to bound away from zero kernel values.\n",
        "      causal: whether attention is causal or not.\n",
        "      projection_matrix_type: None if Identity should be used, otherwise random\n",
        "        projection matrix will be applied.\n",
        "      nb_random_features: number of random features to be used (relevant only if\n",
        "        projection_matrix is not None).\n",
        "    \"\"\"\n",
        "    if hidden_size % num_heads:\n",
        "      raise ValueError(\n",
        "          \"Hidden size ({}) must be divisible by the number of heads ({}).\"\n",
        "          .format(hidden_size, num_heads))\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_dropout = attention_dropout\n",
        "    self.kernel_transformation = kernel_transformation\n",
        "    self.numerical_stabilizer = numerical_stabilizer\n",
        "    self.causal = causal\n",
        "    self.projection_matrix_type = projection_matrix_type\n",
        "    self.nb_random_features = nb_random_features\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"Builds the layer.\"\"\"\n",
        "    # Layers for linearly projecting the queries, keys, and values.\n",
        "    size_per_head = self.hidden_size // self.num_heads\n",
        "\n",
        "    def _glorot_initializer(fan_in, fan_out):\n",
        "      limit = math.sqrt(6.0 / (fan_in + fan_out))\n",
        "      return tf.keras.initializers.RandomUniform(minval=-limit, maxval=limit)\n",
        "\n",
        "    attention_initializer = _glorot_initializer(input_shape.as_list()[-1],\n",
        "                                                self.hidden_size)\n",
        "    self.query_dense_layer = DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"query\")\n",
        "    self.key_dense_layer = DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"key\")\n",
        "    self.value_dense_layer = DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"value\")\n",
        "\n",
        "    output_initializer = _glorot_initializer(self.hidden_size, self.hidden_size)\n",
        "    self.output_dense_layer = DenseEinsum(\n",
        "        output_shape=self.hidden_size,\n",
        "        num_summed_dimensions=2,\n",
        "        kernel_initializer=output_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"output_transform\")\n",
        "    super(Attention, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"hidden_size\": self.hidden_size,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"attention_dropout\": self.attention_dropout,\n",
        "    }\n",
        "\n",
        "  def call(self,\n",
        "           query_input,\n",
        "           source_input,\n",
        "           bias,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    \"\"\"Apply attention mechanism to query_input and source_input.\n",
        "    Args:\n",
        "      query_input: A tensor with shape [batch_size, length_query, hidden_size].\n",
        "      source_input: A tensor with shape [batch_size, length_source,\n",
        "        hidden_size].\n",
        "      bias: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "        the attention bias that will be added to the result of the dot product.\n",
        "      training: A bool, whether in training mode or not.\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape [batch_size, i, heads, dim_per_head],\n",
        "             \"v\": tensor with shape [batch_size, i, heads, dim_per_head]} where\n",
        "               i is the current decoded length for non-padded decode, or max\n",
        "               sequence length for padded decode.\n",
        "      decode_loop_step: An integer, step number of the decoding loop. Used only\n",
        "        for autoregressive inference on TPU.\n",
        "    Returns:\n",
        "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "    \"\"\"\n",
        "    # Linearly project the query, key and value using different learned\n",
        "    # projections. Splitting heads is automatically done during the linear\n",
        "    # projections --> [batch_size, length, num_heads, dim_per_head].\n",
        "    query = self.query_dense_layer(query_input)\n",
        "    key = self.key_dense_layer(source_input)\n",
        "    value = self.value_dense_layer(source_input)\n",
        "\n",
        "    if self.projection_matrix_type is None:\n",
        "      projection_matrix = None\n",
        "    else:\n",
        "      dim = query.shape[-1]\n",
        "      seed = tf.math.ceil(tf.math.abs(tf.math.reduce_sum(query) * BIG_CONSTANT))\n",
        "      seed = tf.dtypes.cast(seed, tf.int32)\n",
        "      projection_matrix = create_projection_matrix(\n",
        "          self.nb_random_features, dim, seed=seed)\n",
        "\n",
        "    if cache is not None:\n",
        "      # Combine cached keys and values with new keys and values.\n",
        "      if decode_loop_step is not None:\n",
        "        cache_k_shape = cache[\"k\"].shape.as_list()\n",
        "        indices = tf.reshape(\n",
        "            tf.one_hot(decode_loop_step, cache_k_shape[1], dtype=key.dtype),\n",
        "            [1, cache_k_shape[1], 1, 1])\n",
        "        key = cache[\"k\"] + key * indices\n",
        "        cache_v_shape = cache[\"v\"].shape.as_list()\n",
        "        indices = tf.reshape(\n",
        "            tf.one_hot(decode_loop_step, cache_v_shape[1], dtype=value.dtype),\n",
        "            [1, cache_v_shape[1], 1, 1])\n",
        "        value = cache[\"v\"] + value * indices\n",
        "      else:\n",
        "        key = tf.concat([tf.cast(cache[\"k\"], key.dtype), key], axis=1)\n",
        "        value = tf.concat([tf.cast(cache[\"v\"], value.dtype), value], axis=1)\n",
        "\n",
        "      # Update cache\n",
        "      cache[\"k\"] = key\n",
        "      cache[\"v\"] = value\n",
        "\n",
        "    attention_output = favor_attention(query, key, value,\n",
        "                                       self.kernel_transformation, self.causal,\n",
        "                                       projection_matrix)\n",
        "    attention_output = self.output_dense_layer(attention_output)\n",
        "    return attention_output\n",
        "\n",
        "\n",
        "class SelfAttention(Attention):\n",
        "  \"\"\"Multiheaded self-attention layer.\"\"\"\n",
        "\n",
        "  def call(self,\n",
        "           query_input,\n",
        "           bias,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    return super(SelfAttention, self).call(query_input, query_input, bias,\n",
        "                                           training, cache, decode_loop_step)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKwlWlt-gWod"
      },
      "source": [
        "\n",
        "SEQUENCE_LENGTH = 196_608\n",
        "BIN_SIZE = 128\n",
        "TARGET_LENGTH = 896\n",
        "\n",
        "\n",
        "class Enformer(tf.keras.Model):\n",
        "  \"\"\"Main model.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               channels: int = 1536,\n",
        "               num_transformer_layers: int = 11,\n",
        "               num_heads: int = 8,\n",
        "               pooling_type: str = 'attention',\n",
        "               name: str = 'enformer'):\n",
        "    \"\"\"Enformer model.\n",
        "    Args:\n",
        "      channels: Number of convolutional filters and the overall 'width' of the\n",
        "        model.\n",
        "      num_transformer_layers: Number of transformer layers.\n",
        "      num_heads: Number of attention heads.\n",
        "      pooling_type: Which pooling function to use. Options: 'attention' or max'.\n",
        "      name: Name of sonnet module.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    # pylint: disable=g-complex-comprehension,g-long-lambda,cell-var-from-loop\n",
        "    heads_channels = {'human': 5313, 'mouse': 1643}\n",
        "    dropout_rate = 0.4\n",
        "    assert channels % num_heads == 0, ('channels needs to be divisible '\n",
        "                                       f'by {num_heads}')\n",
        "    whole_attention_kwargs = {\n",
        "        'hidden_size' : 256,\n",
        "               'num_heads':8,\n",
        "               'attention_dropout' : .2,\n",
        "               'kernel_transformation' : softmax_kernel_transformation,\n",
        "               'numerical_stabilizer' : 0.001,\n",
        "               'causal': False,\n",
        "               'projection_matrix_type' : None,\n",
        "               'nb_random_features': 1024\n",
        "    }\n",
        "\n",
        "    trunk_name_scope = tf.name_scope('trunk')\n",
        "    trunk_name_scope.__enter__()\n",
        "    \n",
        "    def conv_block(filters, width=1, w_init='glorot_uniform', name='conv_block', **kwargs):\n",
        "      return tf.keras.Sequential([\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tfa.layers.GELU(),\n",
        "          tf.keras.layers.Conv1D(filters, width, kernel_initializer=w_init, **kwargs)\n",
        "      ], name=name)\n",
        "\n",
        "    stem = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv1D(channels // 2, 15),\n",
        "        Residual(conv_block(channels // 2, 1, name='pointwise_conv_block')),\n",
        "        pooling_module(pooling_type, pool_size=2),\n",
        "    ], name='stem')\n",
        "\n",
        "    filter_list = exponential_linspace_int(start=channels // 2, end=channels,\n",
        "                                           num=6, divisible_by=128)\n",
        "    conv_tower = tf.keras.Sequential([\n",
        "        tf.keras.Sequential([\n",
        "            conv_block(num_filters, 5),\n",
        "            Residual(conv_block(num_filters, 1, name='pointwise_conv_block')),\n",
        "            pooling_module(pooling_type, pool_size=2),\n",
        "            ],\n",
        "                   name=f'conv_tower_block_{i}')\n",
        "        for i, num_filters in enumerate(filter_list)], name='conv_tower')\n",
        "\n",
        "    # Transformer.\n",
        "    def transformer_mlp():\n",
        "      return tf.keras.Sequential([\n",
        "           tf.keras.layers.LayerNormalization(\n",
        "    axis=-1, epsilon=0.001, center=True, scale=True,\n",
        "    beta_initializer='zeros', gamma_initializer='ones'),\n",
        "          tf.keras.layers.Dense(channels * 2, activation='relu'),\n",
        "          tf.keras.layers.Dropout(rate= dropout_rate),\n",
        "          tf.keras.layers.Dense(channels),\n",
        "         tf.keras.layers.Dropout(rate= dropout_rate)], name='mlp')\n",
        "      \n",
        "    \n",
        "\n",
        "    # transformer = tf.keras.Sequential([\n",
        "    #     tf.keras.Sequential([\n",
        "    #         Residual(tf.keras.Sequential([\n",
        "    #             tf.keras.layers.LayerNormalization(\n",
        "    # axis=-1, epsilon=0.001, center=True, scale=True,\n",
        "    # beta_initializer='zeros', gamma_initializer='ones'),\n",
        "    #             SelfAttention(**whole_attention_kwargs,\n",
        "    #                                                 name=f'attention_{i}'),\n",
        "    #             tf.keras.layers.Dropout(rate= dropout_rate)], name='mha')),\n",
        "    #         Residual(transformer_mlp())], name=f'transformer_block_{i}')\n",
        "    #     for i in range(num_transformer_layers)], name='transformer')\n",
        "    transformer = tf.keras.Sequential([\n",
        "        tf.keras.Sequential([\n",
        "            Residual(tf.keras.Sequential([\n",
        "                tf.keras.layers.LayerNormalization(\n",
        "    axis=-1, epsilon=0.001, center=True, scale=True,\n",
        "    beta_initializer='zeros', gamma_initializer='ones'),\n",
        "                SelfAttention(**whole_attention_kwargs),\n",
        "                tf.keras.layers.Dropout(rate= dropout_rate)], name='mha')),\n",
        "            Residual(transformer_mlp(), name='residual_transformer')])\n",
        "        for i in range(num_transformer_layers)])\n",
        "    \n",
        "    crop_final = TargetLengthCrop1D(TARGET_LENGTH, name='target_input')\n",
        "\n",
        "    final_pointwise = tf.keras.Sequential([\n",
        "        conv_block(channels * 2, 1),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate / 8),\n",
        "        tfa.layers.GELU()], name='final_pointwise')\n",
        "\n",
        "    self._trunk = tf.keras.Sequential([stem,\n",
        "                              conv_tower,\n",
        "                              transformer,\n",
        "                              crop_final,\n",
        "                              final_pointwise],\n",
        "                             name='trunk')\n",
        "    trunk_name_scope.__exit__(None, None, None)\n",
        "\n",
        "    with tf.name_scope('heads'):\n",
        "      self._heads = {\n",
        "          head: tf.keras.Sequential(\n",
        "              [tf.keras.layers.Dense(num_channels, activation='softplus')],\n",
        "              name=f'head_{head}')\n",
        "          for head, num_channels in heads_channels.items()\n",
        "      }\n",
        "    # pylint: enable=g-complex-comprehension,g-long-lambda,cell-var-from-loop\n",
        "\n",
        "  @property\n",
        "  def trunk(self):\n",
        "    return self._trunk\n",
        "\n",
        "  @property\n",
        "  def heads(self):\n",
        "    return self._heads\n",
        "\n",
        "  def call(self, inputs: tf.Tensor,\n",
        "               training: bool) -> Dict[str, tf.Tensor]:\n",
        "    trunk_embedding = self.trunk(inputs, training=training)\n",
        "    return {\n",
        "        head: head_module(trunk_embedding, training=training)\n",
        "        for head, head_module in self.heads.items()\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, SEQUENCE_LENGTH, 4], tf.float32)])\n",
        "  def predict_on_batch(self, x):\n",
        "    \"\"\"Method for SavedModel.\"\"\"\n",
        "    return self(x, training=False)\n",
        "\n",
        "\n",
        "class TargetLengthCrop1D(tf.keras.layers.Layer):\n",
        "  \"\"\"Crop sequence to match the desired target length.\"\"\"\n",
        "\n",
        "  def __init__(self, target_length: int, name='target_length_crop'):\n",
        "    super().__init__(name=name)\n",
        "    self._target_length = target_length\n",
        "\n",
        "  def call(self, inputs):\n",
        "    trim = (inputs.shape[-2] - self._target_length) // 2\n",
        "    if trim < 0:\n",
        "      raise ValueError('inputs longer than target length')\n",
        "\n",
        "    return inputs[..., trim:-trim, :]\n",
        "\n",
        "\n",
        "def pooling_module(kind, pool_size):\n",
        "  \"\"\"Pooling module wrapper.\"\"\"\n",
        "  if kind == 'attention':\n",
        "    return SoftmaxPooling1D(pool_size=pool_size, per_channel=True,\n",
        "                            w_init_scale=2.0)\n",
        "  elif kind == 'max':\n",
        "    return tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')\n",
        "  else:\n",
        "    raise ValueError(f'Invalid pooling kind: {kind}.')\n",
        "\n",
        "\n",
        "class SoftmaxPooling1D(tf.keras.layers.Layer):\n",
        "  \"\"\"Pooling operation with optional weights.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               pool_size: int = 2,\n",
        "               per_channel: bool = False,\n",
        "               w_init_scale: float = 0.0,\n",
        "               name: str = 'softmax_pooling'):\n",
        "    \"\"\"Softmax pooling.\n",
        "    Args:\n",
        "      pool_size: Pooling size, same as in Max/AvgPooling.\n",
        "      per_channel: If True, the logits/softmax weights will be computed for\n",
        "        each channel separately. If False, same weights will be used across all\n",
        "        channels.\n",
        "      w_init_scale: When 0.0 is equivalent to avg pooling, and when\n",
        "        ~2.0 and `per_channel=False` it's equivalent to max pooling.\n",
        "      name: Module name.\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self._pool_size = pool_size\n",
        "    self._per_channel = per_channel\n",
        "    self._w_init_scale = w_init_scale\n",
        "    self._logit_linear = None\n",
        "\n",
        "\n",
        "  def _initialize(self, num_features):\n",
        "    self._logit_linear = tf.keras.layers.Dense(\n",
        "        num_features if self._per_channel else 1,\n",
        "        use_bias=False,  # Softmax is agnostic to shifts.\n",
        "       kernel_initializer =tf.keras.initializers.Identity(\n",
        "    gain=1.0))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    _, length, num_features = inputs.shape\n",
        "    self._initialize(num_features)\n",
        "    print(inputs.shape, )\n",
        "    inputs = tf.reshape(\n",
        "        inputs,\n",
        "        (-1, length // self._pool_size, self._pool_size, num_features))\n",
        "    return tf.reduce_sum(\n",
        "        inputs * tf.nn.softmax(self._logit_linear(inputs), axis=-2),\n",
        "        axis=-2)\n",
        "\n",
        "\n",
        "class Residual(tf.keras.layers.Layer):\n",
        "  \"\"\"Residual block.\"\"\"\n",
        "\n",
        "  def __init__(self, layer,  name='residual'):\n",
        "    super().__init__(name=name)\n",
        "    '''\n",
        "    layer is some tf.keras layer\n",
        "\n",
        "    '''\n",
        "    self._layer = layer\n",
        "\n",
        "  def call(self, inputs: tf.Tensor, training: bool, *args,\n",
        "               **kwargs) -> tf.Tensor:\n",
        "    return inputs + self._layer(inputs, training, *args, **kwargs)\n",
        "\n",
        "\n",
        "def gelu(x: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Applies the Gaussian error linear unit (GELU) activation function.\n",
        "  Using approximiation in section 2 of the original paper:\n",
        "  https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: Input tensor to apply gelu activation.\n",
        "  Returns:\n",
        "    Tensor with gelu activation applied to it.\n",
        "  \"\"\"\n",
        "  return tf.nn.sigmoid(1.702 * x) * x\n",
        "\n",
        "\n",
        "def one_hot_encode(sequence: str,\n",
        "                   alphabet: str = 'ACGT',\n",
        "                   neutral_alphabet: str = 'N',\n",
        "                   neutral_value: Any = 0,\n",
        "                   dtype=np.float32) -> np.ndarray:\n",
        "  \"\"\"One-hot encode sequence.\"\"\"\n",
        "  def to_uint8(string):\n",
        "    return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
        "  hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
        "  hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
        "  hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
        "  hash_table = hash_table.astype(dtype)\n",
        "  return hash_table[to_uint8(sequence)]\n",
        "\n",
        "\n",
        "def exponential_linspace_int(start, end, num, divisible_by=1):\n",
        "  \"\"\"Exponentially increasing values of integers.\"\"\"\n",
        "  def _round(x):\n",
        "    return int(np.round(x / divisible_by) * divisible_by)\n",
        "\n",
        "  base = np.exp(np.log(end / start) / (num - 1))\n",
        "  return [_round(start * base**i) for i in range(num)]\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joqaHqEe1J7f"
      },
      "source": [
        "x = tf.random.uniform(shape=(1,93696,4))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IKRy_yb1S18"
      },
      "source": [
        "model = Enformer(channels=1536, num_transformer_layers=2, pooling_type='max')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LT4mQeY9Vqw"
      },
      "source": [
        "model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G1D6Amh1WSS"
      },
      "source": [
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def _get_random_input():\n",
        "  seq = ''.join(\n",
        "      [random.choice('ACGT') for _ in range(SEQUENCE_LENGTH)])\n",
        "  return np.expand_dims(one_hot_encode(seq), 0).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "def test_enformer():\n",
        "    model = Enformer(channels=1536, num_transformer_layers=11)\n",
        "    inputs = _get_random_input()\n",
        "    outputs = model(inputs, training=True)\n",
        "    print(outputs['human'].shape, outputs['mouse'].shape)\n",
        "    self.assertEqual(outputs['human'].shape, (1, enformer.TARGET_LENGTH, 5313))\n",
        "    self.assertEqual(outputs['mouse'].shape, (1, enformer.TARGET_LENGTH, 1643))\n",
        "\n",
        "\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FkfPaJA3koq",
        "outputId": "dcda29bd-5010-4f65-97a9-09a578b7acc6"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 93696, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1deWG5sljf"
      },
      "source": [
        ""
      ]
    }
  ]
}