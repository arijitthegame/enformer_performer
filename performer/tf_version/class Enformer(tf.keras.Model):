class Enformer(tf.keras.Model):
    def __init__(self,
               channels: int = 1536,
               num_transformer_layers: int = 11,
               out_heads: dict = {'human': 5313,
                                  'mouse': 1643},
               num_heads: int = 8,
               pooling_type: str = 'attention',
               name: str = 'enformer_vanilla'):
    
        super().__init__(name=name)
    
        heads_channels = out_heads
        dropout_rate = 0.4
        assert channels % num_heads == 0, ('channels needs to be divisible ' f'by {num_heads}')
    
        whole_attention_kwargs = {
            'attention_dropout_rate': 0.05,
            'initializer': None,
            'key_size': 64,
            'num_heads': 8,
            'num_relative_position_features': channels // num_heads,
            'positional_dropout_rate': 0.01,
            'relative_position_functions': [
                'positional_features_exponential',
                'positional_features_central_mask',
                'positional_features_gamma'
            ],
            'relative_positions': True,
            'scaling': True,
            'value_size': channels // num_heads,
            'zero_initialize': True
        }